{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GBQ Datapipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this script is to automatically query GBQ in a python script. \n",
    "<br> It allows to send SQL queries to GBQ and returns a pandas df containing the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Authenticate with bigquery and create client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_APPLICATION_CREDENTIALS=GBQ_python_credentials.json\n"
     ]
    }
   ],
   "source": [
    "# import google cloud bigquery library \n",
    "from google.cloud import bigquery\n",
    "\n",
    "# setting google application credentials as environment variable\n",
    "%env GOOGLE_APPLICATION_CREDENTIALS=GBQ_python_credentials.json\n",
    "\n",
    "# initiate a client to google big query \n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Function to call gbq and store result as pandas df (including query limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function 1 - queries gbq and stores result in df\n",
    "def query_to_pandas(client, query):\n",
    "    \"\"\"\n",
    "    sends query to gbq and returns result as pandas df\n",
    "    arguments:\n",
    "    client: gbq client object\n",
    "    query: (str) sql query\n",
    "    \"\"\"\n",
    "    # import libraries\n",
    "    import pandas as pd\n",
    "\n",
    "    # call cliet object with query\n",
    "    results = client.query(query)\n",
    "\n",
    "    # create iterator element\n",
    "    iterator = results.result(timeout=30)\n",
    "    rows = list(iterator)\n",
    "\n",
    "    # Transform the rows into a nice pandas dataframe\n",
    "    df = pd.DataFrame(data=[list(x.values()) for x in rows], columns=list(rows[0].keys()))\n",
    "\n",
    "    # compute memory usage of df\n",
    "    memory_usage = str(df.memory_usage(index=True).sum())\n",
    "    print('size of resulting df (bytes): '+memory_usage)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function 2 - estimates query limit\n",
    "def estimate_query_size(client, query):\n",
    "        \"\"\"\n",
    "        Estimate gigabytes scanned by query.\n",
    "        Does not consider if there is a cached query table.\n",
    "        See https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.dryRun\n",
    "        \"\"\"\n",
    "        my_job_config = bigquery.job.QueryJobConfig()\n",
    "        my_job_config.dry_run = True\n",
    "        my_job = client.query(query, job_config=my_job_config)\n",
    "        return my_job.total_bytes_processed / 2**30\n",
    "\n",
    "\n",
    "\n",
    "# Function 3 - calls function 1 after checking query limit provided by function 2\n",
    "def query_to_pandas_safe(client, query, query_limit_gb):\n",
    "    \"\"\"\n",
    "    Queries only if query size is below query limit.\n",
    "    Returns query as pd df or throws error message. \n",
    "    Calls two functions \n",
    "        (1) estimate query size\n",
    "        (2) query to pandas\n",
    "    \"\"\"\n",
    "    # check query limit\n",
    "    query_size = estimate_query_size(client, query)\n",
    "\n",
    "    if query_size <=  query_limit_gb:\n",
    "        return query_to_pandas(client, query)\n",
    "    else:\n",
    "        msg = \"Query cancelled; estimated size of {0} exceeds limit of {1} GB\"\n",
    "        print(msg.format(query_size, query_limit_gb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Daily transactions per dApp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute daily transaction metrics per dApp\n",
    "# sql query \n",
    "query = \"\"\"\n",
    "WITH dapps AS\n",
    "        (SELECT \n",
    "            string_field_0 d_name,\n",
    "            string_field_1 c_address\n",
    "         FROM `eth-transactions.dapps_contract_link.dapps_contract_link` dapp_contract_link \n",
    "        ),\n",
    "    txn2dapp AS  #contains only transactions to dapps \n",
    "        (SELECT\n",
    "            txn.from_address sender,\n",
    "            txn.to_address ,\n",
    "            dapps.d_name d_name,\n",
    "            txn.value value,\n",
    "            txn.gas gas,\n",
    "            txn.gas_price gas_price,\n",
    "            txn.receipt_gas_used, \n",
    "            txn.block_timestamp block_timestamp\n",
    "         FROM `bigquery-public-data.crypto_ethereum.transactions` AS txn\n",
    "         INNER JOIN dapps\n",
    "         ON txn.to_address = LOWER(dapps.c_address)\n",
    "        )\n",
    "SELECT \n",
    "    DATE(block_timestamp) day,\n",
    "    d_name, \n",
    "    COUNT(*) txn_count,\n",
    "    AVG(gas) avg_gas,\n",
    "    MIN(gas) min_gas,\n",
    "    MAX(gas) max_gas,\n",
    "    AVG(gas_price) avg_gas_price,\n",
    "    MIN(gas_price) min_gas_price,\n",
    "    MAX(gas_price) max_gas_price,\n",
    "    AVG(value) avg_value,\n",
    "    MIN(value) min_value,\n",
    "    MAX(value) max_value,\n",
    "    AVG(receipt_gas_used) avg_receipt_gas_used,\n",
    "    MIN(receipt_gas_used) min_receipt_gas_used,\n",
    "    MAX(receipt_gas_used) max_receipt_gas_used\n",
    "FROM txn2dapp\n",
    "GROUP BY 1,2 \n",
    "ORDER BY 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of resulting df (bytes): 39909608\n"
     ]
    }
   ],
   "source": [
    "# run query \n",
    "daily_txn_metrics = query_to_pandas(client, query)\n",
    "# store result in local json file \n",
    "daily_txn_metrics.to_json(r'daily_dapp_txn_metrics.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Big Query Helper Functions and Additional Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Big Query Helper Functions\n",
    "https://github.com/SohierDane/BigQuery_Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper class to simplify common read-only BigQuery tasks.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "class BigQueryHelper(object):\n",
    "    \"\"\"\n",
    "    Helper class to simplify common BigQuery tasks like executing queries,\n",
    "    showing table schemas, etc without worrying about table or dataset pointers.\n",
    "    See the BigQuery docs for details of the steps this class lets you skip:\n",
    "    https://googlecloudplatform.github.io/google-cloud-python/latest/bigquery/reference.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, active_project, dataset_name, max_wait_seconds=180):\n",
    "        self.project_name = active_project\n",
    "        self.dataset_name = dataset_name\n",
    "        self.max_wait_seconds = max_wait_seconds\n",
    "        self.client = bigquery.Client()\n",
    "        self.__dataset_ref = self.client.dataset(self.dataset_name, project=self.project_name)\n",
    "        self.dataset = None\n",
    "        self.tables = dict()  # {table name (str): table object}\n",
    "        self.__table_refs = dict()  # {table name (str): table reference}\n",
    "        self.total_gb_used_net_cache = 0\n",
    "        self.BYTES_PER_GB = 2**30\n",
    "\n",
    "    def __fetch_dataset(self):\n",
    "        \"\"\"\n",
    "        Lazy loading of dataset. For example,\n",
    "        if the user only calls `self.query_to_pandas` then the\n",
    "        dataset never has to be fetched.\n",
    "        \"\"\"\n",
    "        if self.dataset is None:\n",
    "            self.dataset = self.client.get_dataset(self.__dataset_ref)\n",
    "\n",
    "    def __fetch_table(self, table_name):\n",
    "        \"\"\"\n",
    "        Lazy loading of table\n",
    "        \"\"\"\n",
    "        self.__fetch_dataset()\n",
    "        if table_name not in self.__table_refs:\n",
    "            self.__table_refs[table_name] = self.dataset.table(table_name)\n",
    "        if table_name not in self.tables:\n",
    "            self.tables[table_name] = self.client.get_table(self.__table_refs[table_name])\n",
    "\n",
    "    def __handle_record_field(self, row, schema_details, top_level_name=''):\n",
    "        \"\"\"\n",
    "        Unpack a single row, including any nested fields.\n",
    "        \"\"\"\n",
    "        name = row['name']\n",
    "        if top_level_name != '':\n",
    "            name = top_level_name + '.' + name\n",
    "        schema_details.append([{\n",
    "            'name': name,\n",
    "            'type': row['type'],\n",
    "            'mode': row['mode'],\n",
    "            'fields': pd.np.nan,\n",
    "            'description': row['description']\n",
    "                               }])\n",
    "        # float check is to dodge row['fields'] == np.nan\n",
    "        if type(row.get('fields', 0.0)) == float:\n",
    "            return None\n",
    "        for entry in row['fields']:\n",
    "            self.__handle_record_field(entry, schema_details, name)\n",
    "\n",
    "    def __unpack_all_schema_fields(self, schema):\n",
    "        \"\"\"\n",
    "        Unrolls nested schemas. Returns dataframe with one row per field,\n",
    "        and the field names in the format accepted by the API.\n",
    "        Results will look similar to the website schema, such as:\n",
    "            https://bigquery.cloud.google.com/table/bigquery-public-data:github_repos.commits?pli=1\n",
    "        Args:\n",
    "            schema: DataFrame derived from api repr of raw table.schema\n",
    "        Returns:\n",
    "            Dataframe of the unrolled schema.\n",
    "        \"\"\"\n",
    "        schema_details = []\n",
    "        schema.apply(lambda row:\n",
    "            self.__handle_record_field(row, schema_details), axis=1)\n",
    "        result = pd.concat([pd.DataFrame.from_dict(x) for x in schema_details])\n",
    "        result.reset_index(drop=True, inplace=True)\n",
    "        del result['fields']\n",
    "        return result\n",
    "\n",
    "    def table_schema(self, table_name):\n",
    "        \"\"\"\n",
    "        Get the schema for a specific table from a dataset.\n",
    "        Unrolls nested field names into the format that can be copied\n",
    "        directly into queries. For example, for the `github.commits` table,\n",
    "        the this will return `committer.name`.\n",
    "        This is a very different return signature than BigQuery's table.schema.\n",
    "        \"\"\"\n",
    "        self.__fetch_table(table_name)\n",
    "        raw_schema = self.tables[table_name].schema\n",
    "        schema = pd.DataFrame.from_dict([x.to_api_repr() for x in raw_schema])\n",
    "        # the api_repr only has the fields column for tables with nested data\n",
    "        if 'fields' in schema.columns:\n",
    "            schema = self.__unpack_all_schema_fields(schema)\n",
    "        # Set the column order\n",
    "        schema = schema[['name', 'type', 'mode', 'description']]\n",
    "        return schema\n",
    "\n",
    "    def list_tables(self):\n",
    "        \"\"\"\n",
    "        List the names of the tables in a dataset\n",
    "        \"\"\"\n",
    "        self.__fetch_dataset()\n",
    "        return([x.table_id for x in self.client.list_tables(self.dataset)])\n",
    "\n",
    "    def estimate_query_size(self, query):\n",
    "        \"\"\"\n",
    "        Estimate gigabytes scanned by query.\n",
    "        Does not consider if there is a cached query table.\n",
    "        See https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.dryRun\n",
    "        \"\"\"\n",
    "        my_job_config = bigquery.job.QueryJobConfig()\n",
    "        my_job_config.dry_run = True\n",
    "        my_job = self.client.query(query, job_config=my_job_config)\n",
    "        return my_job.total_bytes_processed / self.BYTES_PER_GB\n",
    "\n",
    "    def query_to_pandas(self, query):\n",
    "        \"\"\"\n",
    "        Execute a SQL query & return a pandas dataframe\n",
    "        \"\"\"\n",
    "        my_job = self.client.query(query)\n",
    "        start_time = time.time()\n",
    "        while not my_job.done():\n",
    "            if (time.time() - start_time) > self.max_wait_seconds:\n",
    "                print(\"Max wait time elapsed, query cancelled.\")\n",
    "                self.client.cancel_job(my_job.job_id)\n",
    "                return None\n",
    "            time.sleep(0.1)\n",
    "        # Queries that hit errors will return an exception type.\n",
    "        # Those exceptions don't get raised until we call my_job.to_dataframe()\n",
    "        # In that case, my_job.total_bytes_billed can be called but is None\n",
    "        if my_job.total_bytes_billed:\n",
    "            self.total_gb_used_net_cache += my_job.total_bytes_billed / self.BYTES_PER_GB\n",
    "        return my_job.to_dataframe()\n",
    "\n",
    "    def query_to_pandas_safe(self, query, max_gb_scanned=1):\n",
    "        \"\"\"\n",
    "        Execute a query, but only if the query would scan less than `max_gb_scanned` of data.\n",
    "        \"\"\"\n",
    "        query_size = self.estimate_query_size(query)\n",
    "        if query_size <= max_gb_scanned:\n",
    "            return self.query_to_pandas(query)\n",
    "        msg = \"Query cancelled; estimated size of {0} exceeds limit of {1} GB\"\n",
    "        print(msg.format(query_size, max_gb_scanned))\n",
    "\n",
    "    def head(self, table_name, num_rows=5, start_index=None, selected_columns=None):\n",
    "        \"\"\"\n",
    "        Get the first n rows of a table as a DataFrame.\n",
    "        Does not perform a full table scan; should use a trivial amount of data as long as n is small.\n",
    "        \"\"\"\n",
    "        self.__fetch_table(table_name)\n",
    "        active_table = self.tables[table_name]\n",
    "        schema_subset = None\n",
    "        if selected_columns:\n",
    "            schema_subset = [col for col in active_table.schema if col.name in selected_columns]\n",
    "        results = self.client.list_rows(active_table, selected_fields=schema_subset,\n",
    "            max_results=num_rows, start_index=start_index)\n",
    "        results = [x for x in results]\n",
    "        return pd.DataFrame(\n",
    "            data=[list(x.values()) for x in results], columns=list(results[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter the public dataset you want to query and an active project\n",
    "bq_assistant = BigQueryHelper(\"eth-transactions\", \"bigquery-public-data.crypto_ethereum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quering GBQ with BigQueryHelper\n",
    "    \n",
    "    # BigQueryHelper Functions\n",
    "# Queries dataset and returns a panda dataframe\n",
    "df = bq_assistant.query_to_pandas(query)\n",
    "\n",
    "# Queries dataset and returns a panda dataframe + allows to set a max scan limit\n",
    "df = bq_assistant.query_to_pandas_safe(query, max_gb_scanned=40)\n",
    "\n",
    "# Lists all tables in the dataset\n",
    "bq_assistant.list_tables()\n",
    "\n",
    "# Shows the head of a specific table\n",
    "bq_assistant.head(\"table_name\", num_rows=3)\n",
    "\n",
    "# Shows details about colums \n",
    "bq_assistant.table_schema(\"table_name\")\n",
    "\n",
    "# check estimated size of a query\n",
    "bq_assistant.estimate_query_size(query)\n",
    "\n",
    "    # other usefull functions\n",
    "# Print size of dataframe\n",
    "print('Size of dataframe: {} Bytes'.format(int(df.memory_usage(index=True, deep=True).sum())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamlet               | 5318\n",
      "kinghenryv           | 5104\n",
      "cymbeline            | 4875\n",
      "troilusandcressida   | 4795\n",
      "kinglear             | 4784\n",
      "kingrichardiii       | 4713\n",
      "2kinghenryvi         | 4683\n",
      "coriolanus           | 4653\n",
      "2kinghenryiv         | 4605\n",
      "antonyandcleopatra   | 4582\n"
     ]
    }
   ],
   "source": [
    "# example bigquery code that queries the shakespeare sample data \n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT corpus AS title, COUNT(word) AS unique_words\n",
    "    FROM `bigquery-public-data.samples.shakespeare`\n",
    "    GROUP BY title\n",
    "    ORDER BY unique_words\n",
    "    DESC LIMIT 10\n",
    "\"\"\"\n",
    "results = client.query(query)\n",
    "\n",
    "for row in results:\n",
    "    title = row['title']\n",
    "    unique_words = row['unique_words']\n",
    "    print(f'{title:<20} | {unique_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example bigquery python code that queries the public github dataset\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT subject AS subject, COUNT(*) AS num_duplicates\n",
    "    FROM bigquery-public-data.github_repos.commits\n",
    "    GROUP BY subject\n",
    "    ORDER BY num_duplicates\n",
    "    DESC LIMIT 10\n",
    "\"\"\"\n",
    "results = client.query(query)\n",
    "\n",
    "for row in results:\n",
    "    subject = row['subject']\n",
    "    num_duplicates = row['num_duplicates']\n",
    "    print(f'{subject:<20} | {num_duplicates:>9,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code that disables caching \n",
    "# code to show query statistics \n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT subject AS subject, COUNT(*) AS num_duplicates\n",
    "    FROM bigquery-public-data.github_repos.commits\n",
    "    GROUP BY subject\n",
    "    ORDER BY num_duplicates\n",
    "    DESC LIMIT 10\n",
    "\"\"\"\n",
    "# disable caching -> \n",
    "job_config = bigquery.job.QueryJobConfig(use_query_cache=False)\n",
    "results = client.query(query, job_config=job_config)\n",
    "\n",
    "for row in results:\n",
    "    subject = row['subject']\n",
    "    num_duplicates = row['num_duplicates']\n",
    "    print(f'{subject:<20} | {num_duplicates:>9,}')\n",
    "\n",
    "# query statistics -> the statistics are provided by the job object\n",
    "print('-'*60)\n",
    "print(f'Created: {results.created}')\n",
    "print(f'Ended:   {results.ended}')\n",
    "print(f'Bytes:   {results.total_bytes_processed:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "name": "python37364bitbasecondab1c05aef7044433bb66977d95ecdfd3f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
